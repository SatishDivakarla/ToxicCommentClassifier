{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Embedding, Input, GlobalMaxPool1D, GlobalAveragePooling1D, concatenate, Reshape\n",
    "from keras.layers import Bidirectional, Dropout, CuDNNGRU, GRU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "import pandas as pd\n",
    "from toxic.nltk_utils import tokenize_sentences\n",
    "from toxic.embedding_utils import read_embedding_list, clear_embedding_list, convert_tokens_to_ids\n",
    "from toxic.nltk_utils import clean\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNKNOWN_WORD = \"_UNK_\"\n",
    "END_WORD = \"_END_\"\n",
    "NAN_WORD = \"_NAN_\"\n",
    "\n",
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "PROBABILITIES_NORMALIZE_COEFFICIENT = 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "train_file_path = \"train.csv\"\n",
    "test_file_path = \"test.csv\"\n",
    "embedding_path = \"crawl-300d-2M.vec\"\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "# train_data['comment_text'] = train_data.apply(lambda x: clean(x.comment_text), axis=1)\n",
    "# train_data['comment_text'] = train_data.apply(lambda x: clean(x.comment_text), axis=1)\n",
    "\n",
    "list_sentences_train = train_data[\"comment_text\"].fillna(NAN_WORD).values\n",
    "list_sentences_test = test_data[\"comment_text\"].fillna(NAN_WORD).values\n",
    "y_train = train_data[CLASSES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments =  159571\n",
      "Total clean comments =  143346\n",
      "Total tags = 35098\n"
     ]
    }
   ],
   "source": [
    "#marking comments without any tags as \"clean\"\n",
    "rowsums=train_data.iloc[:,2:8].sum(axis=1)\n",
    "train_data['clean']=(rowsums==0)\n",
    "#count number of clean entries\n",
    "train_data['clean'].sum()\n",
    "print(\"Total comments = \",len(train_data))\n",
    "print(\"Total clean comments = \",train_data['clean'].sum())\n",
    "print(\"Total tags =\",rowsums.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 107/159571 [00:00<02:29, 1063.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences in train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [01:56<00:00, 1366.49it/s]\n",
      "  0%|          | 143/153164 [00:00<01:47, 1428.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences in test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153164/153164 [01:42<00:00, 1488.13it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing sentences in train set...\")\n",
    "tokenized_sentences_train, words_dict = tokenize_sentences(list_sentences_train, {})\n",
    "\n",
    "print(\"Tokenizing sentences in test set...\")\n",
    "tokenized_sentences_test, words_dict = tokenize_sentences(list_sentences_test, words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl-300d-2M.vec\n",
      "Loading embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999999/1999999 [03:22<00:00, 9888.98it/s] \n"
     ]
    }
   ],
   "source": [
    "print(embedding_path)\n",
    "words_dict[UNKNOWN_WORD] = len(words_dict)\n",
    "\n",
    "print(\"Loading embeddings...\")\n",
    "embedding_list, embedding_word_dict = read_embedding_list(file_path=embedding_path)\n",
    "embedding_size = len(embedding_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "(170075, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Preparing data...\")\n",
    "embedding_list, embedding_word_dict = clear_embedding_list(embedding_list, embedding_word_dict, words_dict)\n",
    "\n",
    "embedding_word_dict[UNKNOWN_WORD] = len(embedding_word_dict)\n",
    "embedding_list.append([0.] * embedding_size)\n",
    "embedding_word_dict[END_WORD] = len(embedding_word_dict)\n",
    "embedding_list.append([-1.] * embedding_size)\n",
    "\n",
    "embedding_matrix = np.array(embedding_list)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170075, 300)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix = np.load(\"embedding_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.backend import manual_variable_initialization\n",
    "# manual_variable_initialization(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for the model\n",
    "sequence_length = 500\n",
    "result_path = \"toxic_results\"\n",
    "batch_size = 256\n",
    "sentences_length = 500\n",
    "recurrent_units=64\n",
    "dropout_rate = 0.3\n",
    "dense_size=32\n",
    "fold_count=10\n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "input_layer = Input(shape=(sequence_length,))\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "x = Bidirectional(GRU(recurrent_units, reset_after=True, recurrent_activation='sigmoid', return_sequences=True))(embedding_layer)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Bidirectional(GRU(recurrent_units, reset_after=True,  recurrent_activation='sigmoid', return_sequences=True))(x)\n",
    "x_max = GlobalMaxPool1D()(x)\n",
    "x_avg = GlobalAveragePooling1D()(x)\n",
    "x = concatenate([x_max, x_avg])\n",
    "output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(clipvalue=1, clipnorm=1), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 300)     51022500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 500, 128)     140544      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 500, 128)     74496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1542        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 51,239,082\n",
      "Trainable params: 216,582\n",
      "Non-trainable params: 51,022,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save_weights(\"model{0}_weights.h5\".format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_0_weights = np.load(\"toxic_results/model0_weights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"model0_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_text = \"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_texts_to_predict= []\n",
    "list_texts_to_predict.append(comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.15it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences_test, words_dict = tokenize_sentences(list_texts_to_predict, words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_to_word = dict((id, word) for word, id in words_dict.items())\n",
    "test_list_of_token_ids = convert_tokens_to_ids(\n",
    "    tokenized_sentences_test,\n",
    "    id_to_word,\n",
    "    embedding_word_dict,\n",
    "    sequence_length)\n",
    "X_test = np.array(test_list_of_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model0_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  2.49350443e-03   1.58164639e-05   4.33413457e-04   4.90666707e-06\n",
      "    1.62080411e-04   3.37695747e-05]]\n",
      "model1_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  1.70183107e-02   1.01795587e-04   9.07640497e-04   4.42239325e-05\n",
      "    1.15616899e-03   7.82819756e-04]]\n",
      "model2_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  4.50579124e-03   2.90278858e-05   2.13885709e-04   1.00334137e-05\n",
      "    3.35724239e-04   1.73899622e-04]]\n",
      "model3_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  1.93699880e-03   2.37136064e-05   2.68978620e-04   7.42457951e-06\n",
      "    1.51936940e-04   2.26853401e-04]]\n",
      "model4_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  4.60684067e-03   3.44349719e-05   5.84497524e-04   1.50917822e-05\n",
      "    1.40265023e-04   9.01440435e-05]]\n",
      "model5_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  1.16769632e-03   7.43807323e-06   5.66191811e-05   1.00185162e-06\n",
      "    1.07531108e-04   3.45301742e-05]]\n",
      "model6_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  1.20389676e-02   3.76691933e-05   1.47664174e-03   1.93812175e-05\n",
      "    1.11709756e-03   2.82585563e-04]]\n",
      "model7_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  1.09359971e-03   1.71058600e-05   3.75986390e-04   1.65000336e-06\n",
      "    1.47524581e-04   7.45195503e-05]]\n",
      "model8_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  1.12063587e-02   3.90624991e-05   5.66444418e-04   5.38830045e-06\n",
      "    4.51631029e-04   1.83527329e-04]]\n",
      "model9_weights.h5\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "(2, -1)\n",
      "[[  1.18286442e-03   1.35800801e-05   1.02482139e-04   7.69388407e-07\n",
      "    6.79426958e-05   4.42693308e-05]]\n"
     ]
    }
   ],
   "source": [
    "test_predicts_list = []\n",
    "for i  in range(0,10):\n",
    "    print(\"model{0}_weights.h5\".format(i))\n",
    "    model.load_weights(\"model{0}_weights.h5\".format(i))\n",
    "    test_predicts = model.predict(X_test, batch_size=1)\n",
    "    print(test_predicts)\n",
    "    test_predicts_list.append(test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  2.49350443e-03,   1.58164639e-05,   4.33413457e-04,\n",
      "          4.90666707e-06,   1.62080411e-04,   3.37695747e-05]], dtype=float32), array([[  1.70183107e-02,   1.01795587e-04,   9.07640497e-04,\n",
      "          4.42239325e-05,   1.15616899e-03,   7.82819756e-04]], dtype=float32), array([[  4.50579124e-03,   2.90278858e-05,   2.13885709e-04,\n",
      "          1.00334137e-05,   3.35724239e-04,   1.73899622e-04]], dtype=float32), array([[  1.93699880e-03,   2.37136064e-05,   2.68978620e-04,\n",
      "          7.42457951e-06,   1.51936940e-04,   2.26853401e-04]], dtype=float32), array([[  4.60684067e-03,   3.44349719e-05,   5.84497524e-04,\n",
      "          1.50917822e-05,   1.40265023e-04,   9.01440435e-05]], dtype=float32), array([[  1.16769632e-03,   7.43807323e-06,   5.66191811e-05,\n",
      "          1.00185162e-06,   1.07531108e-04,   3.45301742e-05]], dtype=float32), array([[  1.20389676e-02,   3.76691933e-05,   1.47664174e-03,\n",
      "          1.93812175e-05,   1.11709756e-03,   2.82585563e-04]], dtype=float32), array([[  1.09359971e-03,   1.71058600e-05,   3.75986390e-04,\n",
      "          1.65000336e-06,   1.47524581e-04,   7.45195503e-05]], dtype=float32), array([[  1.12063587e-02,   3.90624991e-05,   5.66444418e-04,\n",
      "          5.38830045e-06,   4.51631029e-04,   1.83527329e-04]], dtype=float32), array([[  1.18286442e-03,   1.35800801e-05,   1.02482139e-04,\n",
      "          7.69388407e-07,   6.79426958e-05,   4.42693308e-05]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(test_predicts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.58195993e-03   2.50432399e-05   3.45359558e-04   5.63711869e-06\n",
      "    2.43375214e-04   1.18702291e-04]]\n",
      "1.4\n",
      "[[  3.76503496e-04   3.61548629e-07   1.42422981e-05   4.48208460e-08\n",
      "    8.72543094e-06   3.19332220e-06]]\n"
     ]
    }
   ],
   "source": [
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "    \n",
    "test_predicts **= (1. / len(test_predicts_list))\n",
    "print(test_predicts)\n",
    "print(PROBABILITIES_NORMALIZE_COEFFICIENT)\n",
    "test_predicts **= PROBABILITIES_NORMALIZE_COEFFICIENT\n",
    "print(test_predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.76503496e-04   3.61548629e-07   1.42422981e-05   4.48208460e-08\n",
      "    8.72543094e-06   3.19332220e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
