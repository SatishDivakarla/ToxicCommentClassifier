{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from toxic.model import get_model\n",
    "from toxic.nltk_utils import tokenize_sentences\n",
    "from toxic.train_utils import train_folds\n",
    "from toxic.embedding_utils import read_embedding_list, clear_embedding_list, convert_tokens_to_ids\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "\n",
    "from keras.layers import Dense, Embedding, Input, GlobalMaxPool1D, GlobalAveragePooling1D, concatenate, Reshape\n",
    "from keras.layers import Bidirectional, Dropout, CuDNNGRU, GRU\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNKNOWN_WORD = \"_UNK_\"\n",
    "END_WORD = \"_END_\"\n",
    "NAN_WORD = \"_NAN_\"\n",
    "\n",
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "PROBABILITIES_NORMALIZE_COEFFICIENT = 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "train_file_path = \"/Users/sdivakarla/bbanalytics-telemetry-research/satish/SentimentAnalysis/data/WikiToxicity/KaggleToxicDataset/train.csv\"\n",
    "test_file_path = \"/Users/sdivakarla/bbanalytics-telemetry-research/satish/SentimentAnalysis/data/WikiToxicity/KaggleToxicDataset/test.csv\"\n",
    "embedding_path = \"/Users/sdivakarla/bbanalytics-telemetry-research/satish/SentimentAnalysis/model/crawl/crawl-300d-2M.vec\"\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "list_sentences_train = train_data[\"comment_text\"].fillna(NAN_WORD).values\n",
    "list_sentences_test = test_data[\"comment_text\"].fillna(NAN_WORD).values\n",
    "y_train = train_data[CLASSES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments =  159571\n",
      "Total clean comments =  143346\n",
      "Total tags = 35098\n"
     ]
    }
   ],
   "source": [
    "#marking comments without any tags as \"clean\"\n",
    "rowsums=train_data.iloc[:,2:8].sum(axis=1)\n",
    "train_data['clean']=(rowsums==0)\n",
    "#count number of clean entries\n",
    "train_data['clean'].sum()\n",
    "print(\"Total comments = \",len(train_data))\n",
    "print(\"Total clean comments = \",train_data['clean'].sum())\n",
    "print(\"Total tags =\",rowsums.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "def detect_language(row):\n",
    "    try:\n",
    "        return detect(row)\n",
    "    except:\n",
    "        return \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['language'] = train_data['comment_text'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['language'] = test_data['comment_text'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tools.extend_dataset import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "NAN_WORD = \"_NAN_\"\n",
    "\n",
    "\n",
    "def translate_to_english(comment, language):\n",
    "    print(language)\n",
    "    if hasattr(comment, \"decode\"):\n",
    "        comment = comment.decode(\"utf-8\")\n",
    "    if language == \"en\":\n",
    "        print(\"Lang is en\")\n",
    "        return comment\n",
    "    else:\n",
    "        text = TextBlob(comment)\n",
    "        try:\n",
    "            text = text.translate(from_lang=language, to=\"en\")\n",
    "        except NotTranslated:\n",
    "            pass\n",
    "        return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['new_comment_text'] = train_data.apply(lambda x: translate_to_english(x.comment_text, x.language), axis=1)\n",
    "train_data.apply(translate_to_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['new_comment_text'] = train_data.apply(lambda x: translate_to_english(train_data['comment_text'], train_data['language']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Translate the non-english to the english.\n",
    "train_data['comment_text'] = train_data['comment_text'].apply(translate, args=(train_data['language'],))\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(translate, args=(test_data['language'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "train_data_non_english= train_data[train_data['language']!='en']\n",
    "print(len(train_data_non_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(test_data))\n",
    "test_data_non_english= test_data[test_data['language']!='en']\n",
    "print(len(test_data_non_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_non_english.to_csv(\"test_data_non_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=train_data_non_english.iloc[:,2:9].sum()\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"# per class\")\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "plt.xlabel('Type ', fontsize=12)\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Tokenizing sentences in train set...\")\n",
    "tokenized_sentences_train, words_dict = tokenize_sentences(list_sentences_train, {})\n",
    "\n",
    "print(\"Tokenizing sentences in test set...\")\n",
    "tokenized_sentences_test, words_dict = tokenize_sentences(list_sentences_test, words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(embedding_path)\n",
    "words_dict[UNKNOWN_WORD] = len(words_dict)\n",
    "\n",
    "print(\"Loading embeddings...\")\n",
    "embedding_list, embedding_word_dict = read_embedding_list(file_path=embedding_path)\n",
    "embedding_size = len(embedding_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Preparing data...\")\n",
    "embedding_list, embedding_word_dict = clear_embedding_list(embedding_list, embedding_word_dict, words_dict)\n",
    "\n",
    "embedding_word_dict[UNKNOWN_WORD] = len(embedding_word_dict)\n",
    "embedding_list.append([0.] * embedding_size)\n",
    "embedding_word_dict[END_WORD] = len(embedding_word_dict)\n",
    "embedding_list.append([-1.] * embedding_size)\n",
    "\n",
    "embedding_matrix = np.array(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_length = 500\n",
    "result_path = \"toxic_results\"\n",
    "batch_size = 256\n",
    "sentences_length = 500\n",
    "recurrent_units=64\n",
    "dropout_rate = 0.3\n",
    "dense_size=32\n",
    "fold_count=10\n",
    "\n",
    "id_to_word = dict((id, word) for word, id in words_dict.items())\n",
    "train_list_of_token_ids = convert_tokens_to_ids(tokenized_sentences_train,id_to_word,\n",
    "                                                embedding_word_dict,sentences_length)\n",
    "test_list_of_token_ids = convert_tokens_to_ids(tokenized_sentences_test,id_to_word,\n",
    "                                               embedding_word_dict,sentences_length)\n",
    "X_train = np.array(train_list_of_token_ids)\n",
    "X_test = np.array(test_list_of_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_model_func = lambda: get_model(\n",
    "    embedding_matrix,\n",
    "    sentences_length,\n",
    "    dropout_rate,\n",
    "    recurrent_units,\n",
    "    dense_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(sentences_length,))\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "x = Bidirectional(GRU(recurrent_units, return_sequences=True))(embedding_layer)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Bidirectional(GRU(recurrent_units, return_sequences=True))(x)\n",
    "x_max = GlobalMaxPool1D()(x)\n",
    "x_avg = GlobalAveragePooling1D()(x)\n",
    "x = concatenate([x_max, x_avg])\n",
    "#x = Dense(dense_size, activation=\"relu\")(x)\n",
    "output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(clipvalue=1, clipnorm=1), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting to train models...\")\n",
    "models = train_folds(X_train, y_train, fold_count, batch_size, get_model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_path = toxic_results\n",
    "if not os.path.exists(result_path):\n",
    "        os.mkdir(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicting results...\")\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    model_path = os.path.join(args.result_path, \"model{0}_weights.npy\".format(fold_id))\n",
    "    np.save(model_path, model.get_weights())\n",
    "    test_predicts_path = os.path.join(args.result_path, \"test_predicts{0}.npy\".format(fold_id))\n",
    "    test_predicts = model.predict(X_test, batch_size=args.batch_size)\n",
    "    test_predicts_list.append(test_predicts)\n",
    "    np.save(test_predicts_path, test_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predicts = np.ones(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts *= fold_predict\n",
    "    test_predicts **= (1. / len(test_predicts_list))\n",
    "    test_predicts **= PROBABILITIES_NORMALIZE_COEFFICIENT\n",
    "    \n",
    "    test_ids = test_data[\"id\"].values\n",
    "    test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "    test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "    test_predicts[\"id\"] = test_ids\n",
    "    test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "    submit_path = os.path.join(args.result_path, \"submit\")\n",
    "    test_predicts.to_csv(submit_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
